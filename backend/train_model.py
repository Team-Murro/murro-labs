# backend/train_model.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sqlalchemy.orm import Session
from database import SessionLocal
from models import LottoDraw

# --- 1. ì„¤ì • ---
WINDOW_SIZE = 20       # ê³¼ê±° 5íšŒì°¨ë¥¼ ë³´ê³  ë‹¤ìŒì„ ì˜ˆì¸¡
HIDDEN_SIZE = 256     # AI ë‘ë‡Œ í¬ê¸° (í´ìˆ˜ë¡ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
LAYERS = 3            # LSTM ì¸µ ê°œìˆ˜
EPOCHS = 5000         # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (ë§ì„ìˆ˜ë¡ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ ì •í™•í•´ì§ˆ ìˆ˜ ìˆìŒ)
LEARNING_RATE = 0.001

# --- 2. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ ---
def prepare_data():
    db: Session = SessionLocal()
    # 1íšŒë¶€í„° ìµœì‹ íšŒì°¨ê¹Œì§€ ì •ë ¬í•´ì„œ ê°€ì ¸ì˜´
    draws = db.query(LottoDraw).order_by(LottoDraw.turn.asc()).all()
    db.close()

    if not draws:
        print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    # ë°ì´í„°ë¥¼ [0, 0, 1, ... 0] í˜•íƒœì˜ 45ê°œì§œë¦¬ ë²¡í„°ë¡œ ë³€í™˜ (ë²ˆí˜¸ê°€ ë‚˜ì˜¤ë©´ 1, ì•„ë‹ˆë©´ 0)
    # ë¡œë˜ ë²ˆí˜¸ëŠ” 1~45ì´ë¯€ë¡œ ì¸ë±ìŠ¤ 0~44ì— ë§¤í•‘
    data_vectors = []
    for draw in draws:
        vec = [0.0] * 45
        # ë‹¹ì²¨ë²ˆí˜¸ 6ê°œë§Œ ì‚¬ìš© (ë³´ë„ˆìŠ¤ ì œì™¸)
        nums = [draw.num1, draw.num2, draw.num3, draw.num4, draw.num5, draw.num6]
        for n in nums:
            vec[n-1] = 1.0 # í•´ë‹¹ ë²ˆí˜¸ ì¸ë±ìŠ¤ë¥¼ 1ë¡œ ì„¤ì •
        data_vectors.append(vec)
    
    X = [] # ì…ë ¥ (ê³¼ê±° 5ê°œ)
    y = [] # ì •ë‹µ (ë‹¤ìŒ 1ê°œ)

    for i in range(len(data_vectors) - WINDOW_SIZE):
        X.append(data_vectors[i : i + WINDOW_SIZE])
        y.append(data_vectors[i + WINDOW_SIZE])

    # PyTorch í…ì„œë¡œ ë³€í™˜ (GPU ì‚¬ìš© ì¤€ë¹„)
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# --- 3. LSTM ëª¨ë¸ ì •ì˜ ---
class LottoLSTM(nn.Module):
    def __init__(self):
        super(LottoLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size=45, hidden_size=HIDDEN_SIZE, num_layers=LAYERS, batch_first=True)
        self.fc = nn.Linear(HIDDEN_SIZE, 45) # 45ê°œ ë²ˆí˜¸ì— ëŒ€í•œ í™•ë¥  ì¶œë ¥
        self.sigmoid = nn.Sigmoid() # í™•ë¥ ê°’(0~1)ìœ¼ë¡œ ë³€í™˜

    def forward(self, x):
        # x: (batch, window_size, 45)
        out, _ = self.lstm(x) 
        # ë§ˆì§€ë§‰ ì‹œì ì˜ ê²°ê³¼ë§Œ ì‚¬ìš©
        out = out[:, -1, :] 
        out = self.fc(out)
        out = self.sigmoid(out)
        return out

# --- 4. í•™ìŠµ ì‹¤í–‰ ---
def train():
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! ì‚¬ìš© ì¥ì¹˜: {device}")
    if device.type == 'cuda':
        print(f"   GPU ëª¨ë¸: {torch.cuda.get_device_name(0)}")

    X, y = prepare_data()
    if X is None: return

    # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
    X = X.to(device)
    y = y.to(device)

    model = LottoLSTM().to(device)
    criterion = nn.BCELoss() # ë°”ì´ë„ˆë¦¬ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (ê° ë²ˆí˜¸ê°€ ë‚˜ì˜¬ í™•ë¥  ë§ì¶”ê¸°)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    model.train()
    for epoch in range(EPOCHS):
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        if (epoch+1) % 100 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}")

    # ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "lotto_model.pth")
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: lotto_model.pth")

if __name__ == "__main__":
    train()
